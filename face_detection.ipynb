{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f59a5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy(src, dest):\n",
    "    try:\n",
    "        shutil.copytree(src, dest)\n",
    "    except OSError as e:\n",
    "        # If the error was caused because the source wasn't a directory\n",
    "        if e.errno == errno.ENOTDIR:\n",
    "            shutil.copy(src, dest)\n",
    "        else:\n",
    "            print('Directory not copied. Error: %s' % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fdd573",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = '../dataset/'\n",
    "dest = '../LFW/lfw_home'\n",
    "copy(src,dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e087f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying the contents of src and dest folder\n",
    "\n",
    "print(os.listdir('../input'))\n",
    "print(os.listdir('../LFW/lfw_home'))\n",
    "# path = '../LFW/lfw_home/'\n",
    "path = '../LFW/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "lfw_dataset = sklearn.datasets.fetch_lfw_people(data_home = path, min_faces_per_person=100,  download_if_missing = False)\n",
    "\n",
    "#download_if_missing = False ; it prevents downloading, and generates IOError if file is missing, by dfualt value = true\n",
    "# lfw_dataset ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749b1fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, h, w = lfw_dataset.images.shape\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_dataset.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_dataset.target\n",
    "target_names = lfw_dataset.target_names\n",
    "n_classes = target_names.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15da32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a68050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1147a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
    "# dataset): unsupervised feature extraction / dimensionality reduction\n",
    "\n",
    "n_components = 150\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6038c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  #############################################################################\n",
    "# Train a SVM classification model\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "clf = clf.fit(X_train_pca, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f2818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Quantitative evaluation of the model quality on the test set\n",
    "\n",
    "print(\"Predicting people's names on the test set\")\n",
    "t0 = time()\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee38fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Labelled Faces in the Wild Dataset !\n",
    "LFW-People Banner\n",
    "\n",
    "Importing all Libraries...\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "\n",
    "# libraries to support custom function for copying.\n",
    "\n",
    "import errno\n",
    "import shutil\n",
    "Here is THE Big TRICK...\n",
    "By default running the fetch_lfw_people() function downloads the data into the '~/scikit_learn_data' subfolders. This is not a big deal when you run the notebook on your Local PC. But, this becomes Shooting a STAR when you do the same in a KAGGLE KERNEL.\n",
    "\n",
    "Its hard to locate where your data gets downloaded when you run the above function. Matter of relief, I have collected the data from Sklearn's dataset folder, and uploaded here. So, you can easily get to know that data is available at location: '../input'.\n",
    "\n",
    "Again, you get a hit on your nose; Because, You can not fetch and process data there as '../input/lfw_people/' has 'READ ONLY' permission.\n",
    "\n",
    "Solution:\n",
    "I created a folder named '../LFW/lfw_people'. And, set it as the path. So, the next time my fetching function will access and process data here. Then, I copied my complete dataset to this location( Moving is also a great option).\n",
    "\n",
    "Now, It is behaving exactly like its running on your Local PC. Hurray !\n",
    "\n",
    "def copy(src, dest):\n",
    "    try:\n",
    "        shutil.copytree(src, dest)\n",
    "    except OSError as e:\n",
    "        # If the error was caused because the source wasn't a directory\n",
    "        if e.errno == errno.ENOTDIR:\n",
    "            shutil.copy(src, dest)\n",
    "        else:\n",
    "            print('Directory not copied. Error: %s' % e)\n",
    "copying the contents of src folder: files + directory...\n",
    "src = '../input/'\n",
    "dest = '../LFW/lfw_home'\n",
    "copy(src,dest)\n",
    "# verifying the contents of src and dest folder\n",
    "\n",
    "print(os.listdir('../input'))\n",
    "print(os.listdir('../LFW/lfw_home'))\n",
    "# path = '../LFW/lfw_home/'\n",
    "path = '../LFW/'\n",
    "['pairsDevTest.txt', 'lfw-funneled.tgz', 'pairs.txt', 'pairsDevTrain.txt']\n",
    "['pairs.txt', 'pairsDevTest.txt', 'lfw-funneled.tgz', 'pairsDevTrain.txt']\n",
    "Data loading only; Not downloading as its already available to us.\n",
    "# Load data\n",
    "lfw_dataset = sklearn.datasets.fetch_lfw_people(data_home = path, min_faces_per_person=100,  download_if_missing = False)\n",
    "\n",
    "#download_if_missing = False ; it prevents downloading, and generates IOError if file is missing, by dfualt value = true\n",
    "# lfw_dataset = \n",
    "Above Function Returns...\n",
    "dataset : dict-like object with the following attributes:\n",
    "\n",
    "dataset.data : numpy array of shape (13233, 2914)\n",
    "\n",
    "Each row corresponds to a ravelled face image of original size 62 x 47 pixels. Changing the slice_ or resize parameters will change the shape of the output.\n",
    "\n",
    "dataset.images : numpy array of shape (13233, 62, 47)\n",
    "\n",
    "Each row is a face image corresponding to one of the 5749 people in the dataset. Changing the slice_ or resize parameters will change the shape of the output.\n",
    "\n",
    "dataset.target : numpy array of shape (13233,)\n",
    "\n",
    "Labels associated to each face image. Those labels range from 0-5748 and correspond to the person IDs.\n",
    "\n",
    "dataset.DESCR : string\n",
    "\n",
    "Description of the Labeled Faces in the Wild (LFW) dataset.\n",
    "\n",
    "Note:\n",
    "The argument to our function just prunes all people without at least 100 faces, thus reducing the number of classes. Then we can extract our dataset and other auxiliary information. Finally, we split our dataset into training and testing sets.\n",
    "n_samples, h, w = lfw_dataset.images.shape\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_dataset.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_dataset.target\n",
    "target_names = lfw_dataset.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n",
    "Total dataset size:\n",
    "n_samples: 1140\n",
    "n_features: 2914\n",
    "n_classes: 5\n",
    "# #############################################################################\n",
    "# Split into a training set and a test set using a stratified k fold\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)\n",
    "Princpal Component Analysis:\n",
    "-------------------------------------------------\n",
    "One technique of dimensionality reduction is called principal component analysis (PCA). The idea behind PCA is that we want to select the hyperplane such that when all the points are projected onto it, they are maximally spread out. In other words, we want the axis of maximal variance! A potential axis is the x-axis or y-axis, but, in both cases, that’s not the best axis. However, if we pick a line that cuts through our data diagonally, that is the axis where the data would be most spread!\n",
    "\n",
    "PCA\n",
    "\n",
    "The longer blue axis is the correct axis! If we were to project our points onto this axis, they would be maximally spread! But how do we figure out this axis? We can borrow a term from linear algebra called eigenvectors! This is where eigenfaces gets its name!\n",
    "\n",
    "Essentially, we compute the covariance matrix of our data and consider that covariance matrix’s largest eigenvectors. Those are our principal axes and the axes that we project our data onto to reduce dimensions. Using this approach, we can take high-dimensional data and reduce it down to a lower dimension by selecting the largest eigenvectors of the covariance matrix and projecting onto those eigenvectors.\n",
    "\n",
    "Since we’re computing the axes of maximum spread, we’re retaining the most important aspects of our data. It’s easier for our classifier to separate faces when our data are spread out as opposed to bunched together.\n",
    "\n",
    "(There are other dimensionality techniques, such as Linear Discriminant Analysis, that use supervised learning and are also used in face recognition, but PCA works really well!)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
    "# dataset): unsupervised feature extraction / dimensionality reduction\n",
    "\n",
    "n_components = 150\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "Extracting the top 150 eigenfaces from 855 faces\n",
    "done in 0.256s\n",
    "Now we can simply use scikit-learn’s PCA class to perform the dimensionality reduction for us! We have to select the number of components, i.e., the output dimensionality (the number of eigenvectors to project onto), that we want to reduce down to, and feel free to tweak this parameter to try to get the best result! We’ll use 150 components. Additionally, we’ll whiten our data, which is easy to do with a simple boolean flag! (Whitening just makes our resulting data have a unit variance, which has been shown to produce better results).\n",
    "\n",
    "We can apply the transform to bring our images down to a 150-dimensional space.\n",
    "\n",
    "Notice we’re not performing PCA on the entire dataset, only the training data. This is so we can better generalize to unseen data.\n",
    "\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "Projecting the input data on the eigenfaces orthonormal basis\n",
    "done in 0.023s\n",
    "Remember that PCA produces eigenvectors. We can reshape those eigenvectors into images and visualize the eigenfaces.\n",
    "#  #############################################################################\n",
    "# Train a SVM classification model\n",
    "\n",
    "print(\"Fitting the classifier to the training set\")\n",
    "t0 = time()\n",
    "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "clf = clf.fit(X_train_pca, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best estimator found by grid search:\")\n",
    "print(clf.best_estimator_)\n",
    "Fitting the classifier to the training set\n",
    "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
    "  warnings.warn(CV_WARNING, FutureWarning)\n",
    "done in 16.074s\n",
    "Best estimator found by grid search:\n",
    "SVC(C=1000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma=0.005, kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "# #############################################################################\n",
    "# Quantitative evaluation of the model quality on the test set\n",
    "\n",
    "print(\"Predicting people's names on the test set\")\n",
    "t0 = time()\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n",
    "Predicting people's names on the test set\n",
    "done in 0.060s\n",
    "                   precision    recall  f1-score   support\n",
    "\n",
    "     Colin Powell       0.94      0.92      0.93        64\n",
    "  Donald Rumsfeld       0.93      0.78      0.85        32\n",
    "    George W Bush       0.89      0.98      0.93       127\n",
    "Gerhard Schroeder       0.92      0.83      0.87        29\n",
    "       Tony Blair       0.90      0.79      0.84        33\n",
    "\n",
    "         accuracy                           0.91       285\n",
    "        macro avg       0.91      0.86      0.88       285\n",
    "     weighted avg       0.91      0.91      0.90       285\n",
    "\n",
    "[[ 59   1   4   0   0]\n",
    " [  0  25   5   1   1]\n",
    " [  3   0 124   0   0]\n",
    " [  1   0   2  24   2]\n",
    " [  0   1   5   1  26]]\n",
    "# #############################################################################\n",
    "# Qualitative evaluation of the predictions using matplotlib\n",
    "\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the gallery of the most significative eigenfaces\n",
    "\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214caacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result of the prediction on a portion of the test set\n",
    "\n",
    "def title(y_pred, y_test, target_names, i):\n",
    "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n",
    "    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n",
    "    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n",
    "\n",
    "prediction_titles = [title(y_pred, y_test, target_names, i)\n",
    "                     for i in range(y_pred.shape[0])]\n",
    "\n",
    "plot_gallery(X_test, prediction_titles, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb7bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
